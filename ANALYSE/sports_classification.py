# -*- coding: utf-8 -*-
"""Sports_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RdaGpy5LjzFLceytb3nGEMqPe_rOgY9f
"""

# import system libs
import os
import time
import shutil
import pathlib
import itertools

# import data handling tools
import cv2
import numpy as np
import pandas as pd
import seaborn as sns
sns.set_style('darkgrid')
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report

# import Deep learning Libraries
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam, Adamax
from tensorflow.keras.metrics import categorical_crossentropy
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization
from tensorflow.keras import regularizers

# Ignore Warnings
import warnings
warnings.filterwarnings("ignore")

print ('modules loaded')

# Generate data paths with labels
def define_paths(dir):
    filepaths = []
    labels = []

    folds = os.listdir(dir)
    for fold in folds:
        foldpath = os.path.join(dir, fold)
        filelist = os.listdir(foldpath)
        for file in filelist:
            fpath = os.path.join(foldpath, file)
            filepaths.append(fpath)
            labels.append(fold)

    return filepaths, labels

    # Concatenate data paths with labels into one dataframe ( to later be fitted into the model )
def define_df(files, classes):
    Fseries = pd.Series(files, name= 'filepaths')
    Lseries = pd.Series(classes, name='labels')
    return pd.concat([Fseries, Lseries], axis= 1)

    # Function that contain only a directory of data and it is not splitted
def tr_ts_data(tr_dir, ts_dir):
    # train and valid dataframe
    files, classes = define_paths(tr_dir)
    df = define_df(files, classes)
    strat = df['labels']
    train_df, valid_df = train_test_split(df, train_size= 0.8, shuffle= True, random_state= 123, stratify= strat)

    # test dataframe
    files, classes = define_paths(tr_dir)
    test_df = define_df(files, classes)
    return train_df, valid_df, test_df


    # Function that contain train and test directory of data.
def full_data(data_dir):
    # train dataframe
    files, classes = define_paths(data_dir)
    df = define_df(files, classes)
    strat = df['labels']
    train_df, dummy_df = train_test_split(df,  train_size= 0.8, shuffle= True, random_state= 123, stratify= strat)

    # valid and test dataframe
    strat = dummy_df['labels']
    valid_df, test_df = train_test_split(dummy_df,  train_size= 0.5, shuffle= True, random_state= 123, stratify= strat)

    return train_df, valid_df, test_df

# Function to clean the 'Salary Estimate' column
def clean_salary_estimate(salary_str):
    if salary_str == '-1':
        return None, None, None

    # Remove text in parentheses (e.g., '(Glassdoor est.)')
    salary_str = salary_str.split('(')[0].strip()

    # Remove 'K' and '$' if present
    salary_str = salary_str.replace('K', '').replace('$', '')

    if '-' in salary_str:
        min_s, max_s = map(float, salary_str.split('-'))
    else:
        # Handle cases where there might be a single value, or just a number
        try:
            min_s = float(salary_str)
            max_s = float(salary_str)
        except ValueError:
            return None, None, None # Return None if parsing fails

    # Convert 'K' values to actual numbers if they were just removed earlier
    # Assuming the original values like '70-90' meant 70K-90K
    if min_s < 1000 and max_s < 1000: # Heuristic: if numbers are small, assume they were in thousands
        min_s *= 1000
        max_s *= 1000

    avg_s = (min_s + max_s) / 2
    return min_s, max_s, avg_s

# Apply the cleaning function and create new columns
df[['min_salary', 'max_salary', 'avg_salary']] = df['Salary Estimate'].apply(lambda x: pd.Series(clean_salary_estimate(x)))

display(df.head())

# function that contain the three directory of data train, valid, and test
def tr_val_ts_data(tr_dir, val_dir, ts_dir):

    # train dataframe
    files, classes = define_paths(tr_dir)
    train_df = define_df(files, classes)

    # validation dataframe
    files, classes = define_paths(val_dir)
    valid_df = define_df(files, classes)

    # test dataframe
    files, classes = define_paths(ts_dir)
    test_df = define_df(files, classes)

    return train_df, valid_df, test_df

def split_data(tr_dir, val_dir=None, ts_dir=None):
  # No Validation or Test data
    if val_dir == '' and ts_dir == '':
        train_df, valid_df, test_df = full_data(tr_dir)
        return train_df, valid_df, test_df

    # No Validation data
    elif val_dir == '' and ts_dir != '':
        train_df, valid_df, test_df = tr_ts_data(tr_dir, ts_dir)
        return train_df, valid_df, test_df

    # All data existed
    elif val_dir != '' and ts_dir != '':
        train_df, valid_df, test_df = tr_val_ts_data(tr_dir, val_dir, ts_dir)
        return train_df, valid_df, test_df

print('Missing values in df:')
display(df.isnull().sum())

def create_model_data (train_df, valid_df, test_df, batch_size):
  # define model parameters
    img_size = (224, 224)
    channels = 3 # either BGR or Grayscale
    color = 'rgb'
    img_shape = (img_size[0], img_size[1], channels)

    # Recommended : use custom function for test data batch size, else we can use normal batch size.
    ts_length = len(test_df)
    test_batch_size = max(sorted([ts_length // n for n in range(1, ts_length + 1) if ts_length%n == 0 and ts_length/n <= 80]))
    test_steps = ts_length // test_batch_size

    # This function which will be used in image data generator for data augmentation, it just take the image and return it again.
    def scalar(img):
        return img

    tr_gen = ImageDataGenerator(preprocessing_function= scalar, horizontal_flip= True)
    ts_gen = ImageDataGenerator(preprocessing_function= scalar)

    train_gen = tr_gen.flow_from_dataframe( train_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',
                                        color_mode= color, shuffle= True, batch_size= batch_size)

    valid_gen = ts_gen.flow_from_dataframe( valid_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',
                                        color_mode= color, shuffle= True, batch_size= batch_size)

    # Note: we will use custom test_batch_size, and make shuffle= false
    test_gen = ts_gen.flow_from_dataframe( test_df, x_col= 'filepaths', y_col= 'labels', target_size= img_size, class_mode= 'categorical',
                                        color_mode= color, shuffle= False, batch_size= test_batch_size)

    return train_gen, valid_gen, test_gen

def show_images(gen):
  # return classes , images to be displayed
    g_dict = gen.class_indices        # defines dictionary {'class': index}
    classes = list(g_dict.keys())     # defines list of dictionary's kays (classes), classes names : string
    images, labels = next(gen)        # get a batch size samples from the generator

    # calculate number of displayed samples
    length = len(labels)        # length of batch size
    sample = min(length, 25)    # check if sample less than 25 images

    plt.figure(figsize= (20, 20))

    for i in range(sample):
        plt.subplot(5, 5, i + 1)
        image = images[i] / 255       # scales data to range (0 - 255)
        plt.imshow(image)
        index = np.argmax(labels[i])  # get image index
        class_name = classes[index]   # get class of image
        plt.title(class_name, color= 'blue', fontsize= 12)
        plt.axis('off')
    plt.show()

df_model = df[['Salary Estimate', 'min_salary', 'max_salary', 'avg_salary']]
df_model.head()
df_dum  = pd.get_dummies(df_model)
pd.set_option('display.max_rows',None)

df_dum.head()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# Drop rows where avg_salary is NaN, as this is our target variable
df_cleaned = df_dum.dropna(subset=['avg_salary']).copy()

# Define features (X) and target (y)
# For features, we'll use min_salary, max_salary, and the one-hot encoded Salary Estimate columns
# Exclude avg_salary from features as it's the target
feature_columns = [col for col in df_cleaned.columns if col.startswith('Salary Estimate_') or col == 'min_salary' or col == 'max_salary']
X = df_cleaned[feature_columns]
y = df_cleaned['avg_salary']

print(f"Shape of features (X): {X.shape}")
print(f"Shape of target (y): {y.shape}")
display(X.head())

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")

# Standardize features
# Lasso is sensitive to feature scaling, so standardization is important
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Features standardized.")

# Initialize and train the Lasso Regression model
# You can tune the 'alpha' parameter. A larger alpha means more regularization.
lasso_model = Lasso(alpha=0.1, random_state=42) # Starting with a small alpha
lasso_model.fit(X_train_scaled, y_train)

print("Lasso Regression model trained.")

# Make predictions on the test set
y_pred = lasso_model.predict(X_test_scaled)

# Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"\nModel Evaluation:")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"R-squared (R2): {r2:.2f}")

# Display model coefficients
print("\nModel Coefficients (scaled features):")
coefficients = pd.Series(lasso_model.coef_, index=X.columns)
display(coefficients[coefficients != 0].sort_values(ascending=False)) # Show non-zero coefficients

# Load all utility functions into memory
# The content of these cells needs to be executed to make the functions available.
# You would typically run these cells once at the beginning of your notebook.

# Cell: JdqbHLpHwp8j
# (Contains define_paths, define_df, tr_ts_data, full_data)

# Cell: CERm9D6rxfNz
# (Contains tr_val_ts_data)

# Cell: jZkoAepfye1s
# (Contains split_data)

# Cell: Apj6FNPnysCj
# (Contains create_model_data)

# For demonstration, I will simulate running these cells by calling the function definitions,
# but in a real scenario, you would just execute the respective cells in your notebook.
# If you've already run them, this block is redundant but safe.

# This assumes the definitions are correct and the cells are in the notebook.
# If you encounter NameErrors later, ensure the cells containing these definitions are executed.
print("Assuming utility functions are defined in the notebook cells and will be executed.\nIf not, please execute cells JdqbHLpHwp8j, CERm9D6rxfNz, jZkoAepfye1s, Apj6FNPnysCj manually.")
# No explicit re-execution needed here if they are already in the environment by executing previous cells.

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import os

# --- IMPORTANT: Replace these with your actual image data directories ---
# For a full demonstration, these directories should contain image files.
# If they don't, ImageDataGenerator will report errors.

tr_dir = '/content/path/to/your/training_data' # e.g., '/content/chest_xray/train'
val_dir = '/content/path/to/your/validation_data' # e.g., '/content/chest_xray/val'
ts_dir = '/content/path/to/your/test_data' # e.g., '/content/chest_xray/test'

# Create dummy directories and files for demonstration if they don't exist
# This is a placeholder to allow flow_from_dataframe to run without immediate FileNotFoundError
# However, show_images will still fail if it tries to load non-existent image content.

# You might need to create these dummy directories and files manually if you want this code to run without errors
# when defining generators. However, the purpose here is to show the *usage* flow.

# Placeholder for train_df, valid_df, test_df
# In a real scenario, these would be populated by define_paths and define_df using actual image directories.
# Example if you have directories 'class_a' and 'class_b' inside your tr_dir

# To make this runnable without actual image files (for demonstration of flow),
# we'll create simple dummy dataframes. Note: ImageDataGenerator will fail
# if these filepaths don't lead to actual images.

# Check if dummy directories exist, create if not (for flow_from_dataframe to find paths)
for d in [tr_dir, val_dir, ts_dir]:
    if not os.path.exists(d):
        os.makedirs(d, exist_ok=True)
    # Create dummy subdirectories and files if necessary to make flow_from_dataframe happy
    # This part is highly dependent on your data structure. Assuming 2 classes here.
    for c in ['class_0', 'class_1']:
        class_path = os.path.join(d, c)
        if not os.path.exists(class_path):
            os.makedirs(class_path, exist_ok=True)
        # Create a dummy file
        dummy_file_path = os.path.join(class_path, 'dummy_img.jpg')
        if not os.path.exists(dummy_file_path):
            # Create a simple 1x1 black JPEG file
            from PIL import Image
            img = Image.new('RGB', (1, 1), color = 'black')
            img.save(dummy_file_path)


print(f"Attempting to create dataframes from: {tr_dir}, {val_dir}, {ts_dir}")

# You would typically use split_data(tr_dir, val_dir, ts_dir) here.
# Since the directories are placeholders, we'll create minimal dataframes for flow.

# Manually define dummy dataframes to allow create_model_data to be called
# NOTE: ImageDataGenerator will issue warnings/errors if these files don't exist
# or are not valid images when flow_from_dataframe tries to load them.

train_data_list = []
labels_list = os.listdir(tr_dir)
for label in labels_list:
    class_dir = os.path.join(tr_dir, label)
    if os.path.isdir(class_dir):
        for fname in os.listdir(class_dir):
            train_data_list.append({'filepaths': os.path.join(class_dir, fname), 'labels': label})


if not train_data_list:
    print("Warning: No dummy files found or created. Creating minimal placeholder dataframes.")
    # Fallback if dummy file creation failed or directory structure not as expected
    train_df = pd.DataFrame({
        'filepaths': [os.path.join(tr_dir, 'class_0', 'dummy_img.jpg'), os.path.join(tr_dir, 'class_1', 'dummy_img.jpg')],
        'labels': ['class_0', 'class_1']
    })
    valid_df = pd.DataFrame({
        'filepaths': [os.path.join(val_dir, 'class_0', 'dummy_img.jpg')],
        'labels': ['class_0']
    })
    test_df = pd.DataFrame({
        'filepaths': [os.path.join(ts_dir, 'class_1', 'dummy_img.jpg')],
        'labels': ['class_1']
    })
else:
    train_df = pd.DataFrame(train_data_list)
    # For simplicity, using same structure for valid/test. In real case, use actual valid/test paths.
    valid_df = pd.DataFrame({
        'filepaths': [os.path.join(val_dir, 'class_0', 'dummy_img.jpg')],
        'labels': ['class_0']
    })
    test_df = pd.DataFrame({
        'filepaths': [os.path.join(ts_dir, 'class_1', 'dummy_img.jpg')],
        'labels': ['class_1']
    })


print("\n--- Sample of train_df (ensure filepaths point to real images) ---")
display(train_df.head())

batch_size = 8
print(f"\nCreating image data generators with batch_size={batch_size}...")

try:
    train_gen, valid_gen, test_gen = create_model_data(train_df, valid_df, test_df, batch_size)
    print("Image data generators created successfully (may contain warnings about image loading).")
    print(f"Train classes: {train_gen.class_indices}")
except Exception as e:
    print(f"Error creating image data generators: {e}")
    print("Please ensure your image directories exist and contain valid images.")
    train_gen = None # Set to None if creation failed

if 'train_gen' in locals() and train_gen is not None:
    print("Displaying sample images from the training generator...")
    try:
        show_images(train_gen)
    except Exception as e:
        print(f"Error displaying images: {e}")
        print("This might be due to ImageDataGenerator failing to load actual image files from the dummy paths.")
        print("Please replace the placeholder directory paths with your actual image data paths and ensure images exist.")
else:
    print("Image data generator (train_gen) not available or failed to create. Cannot display images.")
    print("Please run the previous cells and ensure your image directories are correctly specified and contain images.")

from sklearn.model_selection import cross_val_score

lm_l = Lasso()
np.mean(cross_val_score(lm_l, X_train, y_train, scoring= 'neg_mean_absolute_error', cv=2))
alpha = []
error = []

for i in range(1,1000):
    alpha.append(i/10)
    lml = Lasso(alpha=(i/100))
    error.append(np.mean(cross_val_score(lml,X_train, y_train, scoring='neg_mean_absolute_error', cv=2)))

plt.plot(alpha,error)

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import cross_val_score
import numpy as np

lm = LinearRegression()
lm.fit(X_train,y_train)

cross_val_score(lm,X_train,y_train, scoring='neg_mean_absolute_error', cv=2)
cross_val_score(lm,X_train,y_train, scoring='neg_mean_absolute_error', cv=2)
# the above output is to skewed so by taking its mean we will be to read
np.mean(cross_val_score(lm,X_train,y_train, scoring='neg_mean_absolute_error', cv=2))

"""# Task
Visualize the distribution of 'avg_salary' using a histogram or Kernel Density Estimate (KDE) plot, display the non-zero coefficients from the Lasso regression model in a bar chart, and create a scatter plot comparing the actual vs. predicted values from a Linear Regression model with an identity line. Finally, summarize the insights gleaned from these visualizations regarding the data and the models.

## Visualize Average Salary Distribution

### Subtask:
Generate a histogram or a Kernel Density Estimate (KDE) plot of the 'avg_salary' column from the cleaned DataFrame (df_cleaned) to show its distribution. Include appropriate labels and a title.

**Reasoning**:
To visualize the distribution of the 'avg_salary' column, I will create a histogram with a Kernel Density Estimate (KDE) plot using the `seaborn` library as requested in the subtask. This will show both the frequency of salary ranges and a smoothed representation of the distribution.
"""

plt.figure(figsize=(10, 6))
sns.histplot(df_cleaned['avg_salary'], kde=True, bins=10, color='skyblue')
plt.title('Distribution of Average Salary', fontsize=16)
plt.xlabel('Average Salary', fontsize=12)
plt.ylabel('Frequency / Density', fontsize=12)
plt.grid(axis='y', alpha=0.75)
plt.tight_layout()
plt.show()

"""**Reasoning**:
The next step in the main task is to display the non-zero coefficients from the Lasso regression model in a bar chart to understand feature importance. I will filter the `coefficients` Series to only include non-zero values and then create a bar plot.


"""

plt.figure(figsize=(12, 7))
non_zero_coefficients = coefficients[coefficients != 0]
sns.barplot(x=non_zero_coefficients.index, y=non_zero_coefficients.values, palette='viridis')
plt.title('Non-Zero Lasso Regression Coefficients', fontsize=16)
plt.xlabel('Features', fontsize=12)
plt.ylabel('Coefficient Value', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.grid(axis='y', alpha=0.75)
plt.tight_layout()
plt.show()

"""## Visualize Linear Regression Predictions

### Subtask:
Generate a scatter plot comparing the actual 'y_test' values against the predicted 'y_pred' values from the Linear Regression model. This will help assess the model's predictive performance. Include an identity line (y=x) for reference, and appropriate labels and title.

**Reasoning**:
To visualize the Linear Regression model's predictions, I will create a scatter plot comparing the actual `y_test` values with the `y_pred_linear` values, including an identity line, labels, and a title as requested.
"""

y_pred_linear = lm.predict(X_test)

plt.figure(figsize=(10, 7))
sns.scatterplot(x=y_test, y=y_pred_linear, alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2) # Identity line
plt.title('Actual vs. Predicted Salary (Linear Regression)', fontsize=16)
plt.xlabel('Actual Average Salary', fontsize=12)
plt.ylabel('Predicted Average Salary', fontsize=12)
plt.grid(True, linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

"""## Final Task

### Subtask:
Provide a summary of the generated visualizations and what insights they offer about the data and the models.

## Summary:

### Q&A
The visualizations offer insights into the distribution of the target variable and the performance and feature importance of the regression models. The average salary distribution helps understand the data's inherent spread. The Lasso regression coefficients highlight which features are most influential in predicting salary, while the actual vs. predicted plot for Linear Regression visually confirms how well the model aligns with true values.

### Data Analysis Key Findings
*   **Average Salary Distribution**: The average salary distribution is displayed by a histogram with a Kernel Density Estimate (KDE). While the exact shape (e.g., skewed, normal) is not explicitly stated, the plot visually represents the frequency and density of different salary ranges, indicating how salaries are distributed within the dataset.
*   **Lasso Regression Coefficients**: A bar chart of non-zero Lasso regression coefficients identifies features that significantly influence the average salary prediction. Features with larger absolute coefficient values have a stronger impact on the predicted salary, and the sign indicates the direction of that influence (positive or negative).
*   **Linear Regression Predictions**: The scatter plot comparing actual vs. predicted average salaries from the Linear Regression model shows the model's predictive performance. The presence of an identity line (y=x) allows for a direct visual assessment: points clustering closely around this line indicate accurate predictions, while deviation suggests areas where the model's performance could be improved.

### Insights or Next Steps
*   Further investigation into the features identified by Lasso regression with high non-zero coefficients could reveal key drivers of average salary, potentially leading to more targeted data collection or feature engineering efforts.
*   The actual vs. predicted plot for Linear Regression can guide further model evaluation. If there are noticeable patterns in the deviations from the identity line (e.g., consistent underprediction at high salaries), it might suggest the need for a more complex model or feature transformations to capture non-linear relationships.
"""